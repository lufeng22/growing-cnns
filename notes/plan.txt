Current sprint:
Join weighting:
- Learned free weighting
- Learned softmax weighting
- Sliding weighting
Join mode:
- Weighted average
- Probabilistic sample
Setting for randomly initialized new layers

Functionality cards:
Options for growth speed
Loss convergence (rename current lossConvergence to earlyStopping)
Different block types

Errand cards:
Get training to run in Colab
Add ImageNet + other datasets
Dynamic batch sizes across growth steps using model size estimation (https://github.com/jacobkimmel/pytorch_modelsize)
Clean/break up main
Parallelize across multiple GPUs
Fix argument structure for training/evaluation
Convert to package and fix imports
Add option for CPU only
Add clock time to logs

Notes for tomorrow:
Make sure join weights are trainable (may need torch functional interface)
