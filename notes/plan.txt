Functionality cards:
Options for growth speed
Loss convergence (rename current lossConvergence to earlyStopping)
Different block types

Errand cards:
Get training to run in Colab
Add ImageNet + other datasets
Dynamic batch sizes across growth steps using model size estimation (https://github.com/jacobkimmel/pytorch_modelsize)
Clean/break up main
Parallelize across multiple GPUs
Fix argument structure for training/evaluation
Convert to package and fix imports
Add option for CPU only
Add clock time to logs
