Current sprint:
Join weighting:
- Learned free weighting
- Learned softmax weighting
- Sliding weighting
Join mode:
- Weighted average
- Probabilistic sample
Setting for randomly initialized new layers

Functionality cards:
Loss convergence (rename current lossConvergence to earlyStopping)
Growth step transfers weights with best validation performance
Different block types

Errand cards:
Convert to package
Get training to run in Colab
Add ImageNet + other datasets
Dynamic batch sizes across growth steps using model size estimation (https://github.com/jacobkimmel/pytorch_modelsize)
Clean/break up main
Parallelize across multiple GPUs
Fix argument structure for training/evaluation
Convert to package and fix imports
Add option for CPU only
Add clock time to logs

Notes for tomorrow:
Gotta add tests for preserving the function across growth steps with different join types (especially for learned/random join weights)
Gotta add comparing the join weights across growth steps with different join types
Gotta add functionality to make newly joined input the weighted average of old input and new one
