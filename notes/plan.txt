Current sprint:
Join weighting:
- Learned free weighting
- Learned softmax weighting
- Sliding weighting
Join mode:
- Weighted average
- Probabilistic sample
Setting for randomly initialized new layers

Functionality cards:
Loss convergence (rename current lossConvergence to earlyStopping)
Growth step transfers weights with best validation performance
Different block types

Errand cards:
Get training to run in Colab
Add ImageNet + other datasets
Dynamic batch sizes across growth steps using model size estimation (https://github.com/jacobkimmel/pytorch_modelsize)
Clean/break up main
Parallelize across multiple GPUs
Fix argument structure for training/evaluation
Convert to package and fix imports
Add option for CPU only
Add clock time to logs

Notes for tomorrow:
Set softmax weights using joinPreserve correctly?
Add tests for softmax/free join weights that are not equal to initialized version, or with learned weights
Make sure join weights are trainable (may need torch functional interface)
