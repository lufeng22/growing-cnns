Current sprint:
Get training to run in Colab
Copy batch norm parameters from previous layer
Join weighting:
- Learned free weighting
- Learned softmax weighting
- Sliding weighting
Join mode:
- Weighted average
- Probabilistic sample
Setting for randomly initialized new layers

Functionality cards:
Loss convergence (rename current lossConvergence to earlyStopping)
Growth step transfers weights with best validation performance
Different block types

Errand cards:
Add ImageNet + other datasets
Dynamic batch sizes across growth steps using model size estimation (https://github.com/jacobkimmel/pytorch_modelsize)
Clean/break up main
Clean/break up testGrowthController.py
Parallelize across multiple GPUs
Fix argument structure for training/evaluation
Convert to package and fix imports
Add option for CPU only
